{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc9be56",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b3b52",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/jaa/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/jaa/Library/Python/3.9/lib/python/site-packages (1.25.2)\n",
      "Requirement already satisfied: pandas in /Users/jaa/Library/Python/3.9/lib/python/site-packages (2.1.0)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post11.tar.gz (3.6 kB)\n",
      "Requirement already satisfied: tensorflow in /Users/jaa/Library/Python/3.9/lib/python/site-packages (2.14.0)\n",
      "Requirement already satisfied: click in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.25.0)\n",
      "Requirement already satisfied: packaging in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.3)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post11-py3-none-any.whl size=2369 sha256=fa48e8df6d96d7205d110de0430a6dd115988f5f41f6d495e9ba50e9c3b9fcb6\n",
      "  Stored in directory: /Users/jaa/Library/Caches/pip/wheels/9e/9e/4c/184e84f4ce918378a9ec9adafd1b6b73bea45f0a4a7855b6ce\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post11\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk numpy pandas sklearn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67da3bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                   0\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n",
      "1  story of a man who has unnatural feelings for ...\n",
      "2  homelessness  or houselessness as george carli...\n",
      "3  airport    starts as a brand new luxury    pla...\n",
      "4  brilliant over  acting by lesley ann warren . ...\n",
      "          0\n",
      "0  positive\n",
      "1  negative\n",
      "2  positive\n",
      "3  negative\n",
      "4  positive\n",
      "                                                        0\n",
      "count                                               25000\n",
      "unique                                              24902\n",
      "top     this show comes up with interesting locations ...\n",
      "freq                                                    3\n",
      "               0\n",
      "count      25000\n",
      "unique         2\n",
      "top     positive\n",
      "freq       12500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('../data/reviews.txt', header=None)\n",
    "labels = pd.read_csv('../data/labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(type(reviews))\n",
    "print(reviews.head())\n",
    "print(labels.head())\n",
    "print(reviews.describe())\n",
    "print(labels.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 25000\n",
      "Slimmed down size: 6250\n",
      "                                                        0\n",
      "count                                                6250\n",
      "unique                                               6238\n",
      "top     when i got this movie free from my job  along ...\n",
      "freq                                                    3\n",
      "               0\n",
      "count       6250\n",
      "unique         2\n",
      "top     positive\n",
      "freq        3125\n"
     ]
    }
   ],
   "source": [
    "percentage = int(len(reviews) * 0.25)\n",
    "\n",
    "reviews_slim = reviews.iloc[:percentage]\n",
    "labels_slim = labels.iloc[:percentage]\n",
    "Y_slim = Y.iloc[:percentage]\n",
    "\n",
    "# Checking the results\n",
    "print(f\"Original size: {len(reviews)}\")\n",
    "print(f\"Slimmed down size: {len(reviews_slim)}\")\n",
    "\n",
    "print(reviews_slim.describe())\n",
    "print(labels_slim.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_slim.to_csv('../exports/reviews_slim.csv', index=True)\n",
    "labels_slim.to_csv('../exports/labels_slim.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982b946",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review Sentiment\n",
      "0  bromwell high is a cartoon comedy . it ran at ...  positive\n",
      "1  story of a man who has unnatural feelings for ...  negative\n",
      "2  homelessness  or houselessness as george carli...  positive\n",
      "3  airport    starts as a brand new luxury    pla...  negative\n",
      "4  brilliant over  acting by lesley ann warren . ...  positive\n",
      "                                                   Review Sentiment\n",
      "count                                               25000     25000\n",
      "unique                                              24902         2\n",
      "top     this show comes up with interesting locations ...  positive\n",
      "freq                                                    3     12500\n"
     ]
    }
   ],
   "source": [
    "# combining the data to make it easier to work with before splitting again\n",
    "\n",
    "# Remove any unnamed index columns if they exist\n",
    "reviews.columns = reviews.columns.astype(str)\n",
    "labels.columns = labels.columns.astype(str)\n",
    "\n",
    "reviews = reviews.loc[:, ~reviews.columns.str.contains('^Unnamed')]\n",
    "labels = labels.loc[:, ~labels.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Concatenate the two dataframes along the columns\n",
    "combined_df = pd.concat([reviews, labels], axis=1)\n",
    "\n",
    "# Rename the columns for clarity\n",
    "combined_df.columns = ['Review', 'Sentiment']\n",
    "\n",
    "\n",
    "print(combined_df.head())\n",
    "print(combined_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jaa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jaa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Processed_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high is a cartoon comedy . it ran at ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>bromwell high cartoon comedy ran time program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>story man unnatural feeling pig start opening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "      <td>positive</td>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "      <td>negative</td>\n",
       "      <td>airport start brand new luxury plane loaded va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brilliant over  acting by lesley ann warren . ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>brilliant acting lesley ann warren best dramat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Sentiment  \\\n",
       "0  bromwell high is a cartoon comedy . it ran at ...  positive   \n",
       "1  story of a man who has unnatural feelings for ...  negative   \n",
       "2  homelessness  or houselessness as george carli...  positive   \n",
       "3  airport    starts as a brand new luxury    pla...  negative   \n",
       "4  brilliant over  acting by lesley ann warren . ...  positive   \n",
       "\n",
       "                                    Processed_Review  \n",
       "0  bromwell high cartoon comedy ran time program ...  \n",
       "1  story man unnatural feeling pig start opening ...  \n",
       "2  homelessness houselessness george carlin state...  \n",
       "3  airport start brand new luxury plane loaded va...  \n",
       "4  brilliant acting lesley ann warren best dramat...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text preprocessing - tokenisation, removing stop words, lemmatising \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing to each review\n",
    "preprocessed_df = combined_df.copy()\n",
    "\n",
    "preprocessed_df['Processed_Review'] = preprocessed_df['Review'].apply(preprocess_text)\n",
    "\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 25000\n",
      "Slimmed down size: 6250\n"
     ]
    }
   ],
   "source": [
    "percentage = int(len(preprocessed_df) * 0.25)\n",
    "\n",
    "preprocessed_df_slim = preprocessed_df.iloc[:percentage]\n",
    "Y_slim = Y.iloc[:percentage]\n",
    "\n",
    "# Checking the results\n",
    "print(f\"Original size: {len(preprocessed_df)}\")\n",
    "print(f\"Slimmed down size: {len(preprocessed_df_slim)}\")\n",
    "\n",
    "preprocessed_df_slim.to_csv('../exports/reviews_slim_combined_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2148673 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorisation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF Vectorizer instance\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "X = tfidf_vectorizer.fit_transform(preprocessed_df['Processed_Review'])\n",
    "\n",
    "# Checking the shape of the resulting matrix\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encoding the sentiment labels\n",
    "y = label_encoder.fit_transform(preprocessed_df['Sentiment'])\n",
    "\n",
    "# Checking the first few encoded labels\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 5000) (4000, 5000) (5000, 5000)\n",
      "(16000,) (4000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "# Train / Validation / Test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Checking the shapes of the splits\n",
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Further splitting the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Checking the shapes of the new splits\n",
    "print(X_train.shape, X_val.shape, X_test.shape) \n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 10000), (4000, 10000), (5000, 10000), (16000,), (4000,), (5000,))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redoing the train/validate/test split directly on the DataFrame to maintain indices\n",
    "# this is an alternative (the way the assignment says the features/labels should be created)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "preprocessed_df['Sentiment'] = preprocessed_df['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "train_df, test_df = train_test_split(preprocessed_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training, validation, and test data\n",
    "X_train_bow = count_vectorizer.fit_transform(train_df['Processed_Review'])\n",
    "X_val_bow = count_vectorizer.transform(val_df['Processed_Review'])\n",
    "X_test_bow = count_vectorizer.transform(test_df['Processed_Review'])\n",
    "\n",
    "# Also, get the labels for the train, validation, and test sets\n",
    "y_train = train_df['Sentiment']\n",
    "y_val = val_df['Sentiment']\n",
    "y_test = test_df['Sentiment']\n",
    "\n",
    "# Checking the shape of the resulting matrices and label vectors\n",
    "(X_train_bow.shape, X_val_bow.shape, X_test_bow.shape, y_train.shape, y_val.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_bow\n",
    "X_val = X_val_bow\n",
    "X_test = X_test_bow\n",
    "\n",
    "# The model needs this\n",
    "X_train_dense = X_train.toarray()\n",
    "X_val_dense = X_val.toarray()\n",
    "X_test_dense = X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf07ee9",
   "metadata": {},
   "source": [
    "**(b)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2638fce",
   "metadata": {},
   "source": [
    "**(c)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 4s 13ms/step - loss: 1.1152 - accuracy: 0.8265 - val_loss: 0.7791 - val_accuracy: 0.8480\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.8053 - accuracy: 0.8421 - val_loss: 0.8182 - val_accuracy: 0.8543\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.8103 - accuracy: 0.8475 - val_loss: 0.7693 - val_accuracy: 0.8565\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.8069 - accuracy: 0.8481 - val_loss: 0.7948 - val_accuracy: 0.8650\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.7985 - accuracy: 0.8509 - val_loss: 0.7610 - val_accuracy: 0.8558\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.7798 - accuracy: 0.8519 - val_loss: 0.8162 - val_accuracy: 0.8587\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.7832 - accuracy: 0.8531 - val_loss: 0.7760 - val_accuracy: 0.8685\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.7626 - accuracy: 0.8584 - val_loss: 0.7831 - val_accuracy: 0.8675\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.7628 - accuracy: 0.8565 - val_loss: 0.8015 - val_accuracy: 0.8553\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.7532 - accuracy: 0.8597 - val_loss: 0.7608 - val_accuracy: 0.8587\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.7378 - accuracy: 0.8626 - val_loss: 0.7546 - val_accuracy: 0.8692\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.7237 - accuracy: 0.8648 - val_loss: 0.7492 - val_accuracy: 0.8660\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.7214 - accuracy: 0.8676 - val_loss: 0.7221 - val_accuracy: 0.8680\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.7170 - accuracy: 0.8644 - val_loss: 0.7240 - val_accuracy: 0.8670\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.7133 - accuracy: 0.8652 - val_loss: 0.7238 - val_accuracy: 0.8690\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.6989 - accuracy: 0.8691 - val_loss: 0.7397 - val_accuracy: 0.8665\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.6972 - accuracy: 0.8684 - val_loss: 0.7401 - val_accuracy: 0.8648\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.6917 - accuracy: 0.8710 - val_loss: 0.7356 - val_accuracy: 0.8615\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.6723 - accuracy: 0.8720 - val_loss: 0.7041 - val_accuracy: 0.8708\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.6716 - accuracy: 0.8721 - val_loss: 0.7188 - val_accuracy: 0.8648\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.6588 - accuracy: 0.8770 - val_loss: 0.7003 - val_accuracy: 0.8675\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 3s 11ms/step - loss: 0.6563 - accuracy: 0.8739 - val_loss: 0.6973 - val_accuracy: 0.8685\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.6541 - accuracy: 0.8767 - val_loss: 0.6917 - val_accuracy: 0.8683\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.6472 - accuracy: 0.8781 - val_loss: 0.6760 - val_accuracy: 0.8677\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 3s 13ms/step - loss: 0.6388 - accuracy: 0.8823 - val_loss: 0.6721 - val_accuracy: 0.8737\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=10000, kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with the learning rate scheduler as a callback\n",
    "history = model.fit(X_train_dense, y_train, epochs=25, batch_size=64, validation_data=(X_val_dense, y_val), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 5s 19ms/step - loss: 0.5942 - accuracy: 0.8895 - val_loss: 0.6494 - val_accuracy: 0.8745\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 5s 19ms/step - loss: 0.6007 - accuracy: 0.8886 - val_loss: 0.6605 - val_accuracy: 0.8687\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.5924 - accuracy: 0.8917 - val_loss: 0.6449 - val_accuracy: 0.8740\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.6000 - accuracy: 0.8881 - val_loss: 0.6692 - val_accuracy: 0.8600\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.5784 - accuracy: 0.8921 - val_loss: 0.6406 - val_accuracy: 0.8737\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 5s 19ms/step - loss: 0.5739 - accuracy: 0.8913 - val_loss: 0.6321 - val_accuracy: 0.8748\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 4s 18ms/step - loss: 0.5822 - accuracy: 0.8885 - val_loss: 0.6456 - val_accuracy: 0.8733\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 5s 19ms/step - loss: 0.5895 - accuracy: 0.8903 - val_loss: 0.6416 - val_accuracy: 0.8748\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 5s 18ms/step - loss: 0.5795 - accuracy: 0.8937 - val_loss: 0.6488 - val_accuracy: 0.8700\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 4s 18ms/step - loss: 0.5756 - accuracy: 0.8915 - val_loss: 0.6469 - val_accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "# Continue training the model for additional epochs\n",
    "additional_epochs = 10 \n",
    "history_continued = model.fit(X_train_dense, y_train, epochs=additional_epochs, batch_size=64, validation_data=(X_val_dense, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd327a6",
   "metadata": {},
   "source": [
    "**(d)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 0.6782 - accuracy: 0.8736\n",
      "Test Loss: 0.6782421469688416\n",
      "Test Accuracy: 0.8736000061035156\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Evaluate the model on the validation set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_dense, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# first try (20 epochs)\n",
    "# Test Loss: 0.6639389395713806\n",
    "# Test Accuracy: 0.8687499761581421\n",
    "\n",
    "# second try with exponential LR (actually lowered the score a bit)\n",
    "# Validation Loss: 1.0084891319274902\n",
    "# Validation Accuracy: 0.8654000163078308\n",
    "\n",
    "# 3. tried a more gradual custom LR - even lower\n",
    "# Test Loss: 1.0994940996170044\n",
    "# Test Accuracy: 0.8503999710083008\n",
    "\n",
    "# 4. removing custom LR and changing the l2 value Dense(512, activation='relu', input_dim=10000, kernel_regularizer=l2(0.01)) (now with 25 epochs)\n",
    "# Test Loss: 0.6812025904655457\n",
    "# Test Accuracy: 0.8708000183105469\n",
    "\n",
    "# 5. doubling the neurons model.add(Dense(1024, activation='relu', input_dim=10000, kernel_regularizer=l2(0.01))) - 25 epochs\n",
    "# Test Loss: 0.6491168737411499\n",
    "# Test Accuracy: 0.8687999844551086\n",
    "# so a little bit lower score actually but it seemed to ramp up in the later epochs, so im going to continue for 10 more\n",
    "\n",
    "# 5. with 10 more epochs: \n",
    "# Test Loss: 0.6418853998184204\n",
    "# Test Accuracy: 0.8669999837875366\n",
    "# not much better - lets try 10 more\n",
    "# now even a little bit lower\n",
    "\n",
    "# 6. changing the l2 regularizer - model.add(Dense(1024, activation='relu', input_dim=10000, kernel_regularizer=l2(0.03))) (35epochs)\n",
    "# Test Loss: 0.7511987090110779\n",
    "# Test Accuracy: 0.8569999933242798\n",
    "# score lowering - lets try 0.01 regularizer and more neurons\n",
    "\n",
    "# 7. with more neurons the accuracy has improved but not from the 512 with same l2 - \n",
    "# model.add(Dense(2048, activation='relu', input_dim=10000, kernel_regularizer=l2(0.01)))\n",
    "# Test Loss: 0.6616639494895935\n",
    "# Test Accuracy: 0.8708000183105469\n",
    "\n",
    "# 8. increased regularizer to 0.03 decreased accuracy\n",
    "# Test Loss: 0.7975600361824036\n",
    "# Test Accuracy: 0.8597999811172485\n",
    "\n",
    "# concluding from this - the -Dense(512, activation='relu', input_dim=10000, kernel_regularizer=l2(0.01)) - model is the best performance/training burden ratio\n",
    "# Test Loss: 0.6782421469688416\n",
    "# Test Accuracy: 0.8736000061035156"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44ee62",
   "metadata": {},
   "source": [
    "**(e)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on own stuff\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    processed_text = preprocess_text(text)\n",
    "\n",
    "    vectorized_text = count_vectorizer.transform([processed_text]).toarray()\n",
    "\n",
    "    prediction = model.predict(vectorized_text)\n",
    "\n",
    "    sentiment = \"positive\" if prediction[0][0] > 0.5 else \"negative\"\n",
    "    confidence = prediction[0][0] if sentiment == \"positive\" else 1 - prediction[0][0]\n",
    "\n",
    "    return sentiment, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "The predicted sentiment for \"I really enjoyed this movie!\" is positive with a confidence of 0.74.\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "The predicted sentiment for \"This movie was an absolute masterpiece, with stunning visuals and a gripping story.\" is positive with a confidence of 0.77.\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "The predicted sentiment for \"I found the film to be incredibly boring and a waste of time.\" is negative with a confidence of 0.89.\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "The predicted sentiment for \"The acting was mediocre at best and the plot was predictable.\" is negative with a confidence of 0.61.\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "The predicted sentiment for \"An exhilarating and heartwarming adventure I would watch again!\" is positive with a confidence of 0.62.\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "The predicted sentiment for \"Not bad, but not particularly memorable or innovative.\" is negative with a confidence of 0.64.\n"
     ]
    }
   ],
   "source": [
    "sentences = [ \n",
    "        \"I really enjoyed this movie!\",\n",
    "        \"This movie was an absolute masterpiece, with stunning visuals and a gripping story.\",\n",
    "        \"I found the film to be incredibly boring and a waste of time.\",\n",
    "        \"The acting was mediocre at best and the plot was predictable.\",\n",
    "        \"An exhilarating and heartwarming adventure I would watch again!\",\n",
    "        \"Not bad, but not particularly memorable or innovative.\",\n",
    "    ]\n",
    "\n",
    "for sentence in sentences:\n",
    "    predicted_sentiment, confidence = predict_sentiment(sentence)\n",
    "    print(f\"The predicted sentiment for \\\"{sentence}\\\" is {predicted_sentiment} with a confidence of {confidence:.2f}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
