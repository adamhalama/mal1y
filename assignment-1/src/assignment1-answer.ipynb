{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment deals with the most recent Airbnb listings in Copenhagen. The data is collected from [Inside Airbnb](http://insideairbnb.com/copenhagen). Feel free to explore the website further in order to better understand the data. The data (*listings.csv*) has been collected as raw data and needs to be preprocessed.\n",
    "\n",
    "**Hand-in:** Hand in as a group in Itslearning in a **single**, well-organized and easy-to-read Jupyter Notebook. If your group consists of students from different classes, upload in **both** classes.\n",
    "\n",
    "1. First we need to remove all the redundant columns. Please keep the following 22 columns and remove all others:\n",
    "\n",
    "    id\\\n",
    "    name  \n",
    "    host_id  \n",
    "    host_name  \n",
    "    neighbourhood_cleansed  \n",
    "    latitude  \n",
    "    longitude  \n",
    "    room_type  \n",
    "    price  \n",
    "    minimum_nights  \n",
    "    number_of_reviews  \n",
    "    last_review  \n",
    "    review_scores_rating  \n",
    "    review_scores_accuracy  \n",
    "    review_scores_cleanliness  \n",
    "    review_scores_checkin  \n",
    "    review_scores_communication  \n",
    "    review_scores_location  \n",
    "    review_scores_value  \n",
    "    reviews_per_month  \n",
    "    calculated_host_listings_count  \n",
    "    availability_365\n",
    "\n",
    "\n",
    "\n",
    "2. Next we have to handle missing values. Remove all rows where `number_of_reviews = 0`. If there are still missing values, remove the rows that contain them so you have a data set with no missing values.\n",
    "\n",
    "3. Fix the `neighbourhood_cleansed` values (some are missing 'æ ø å'), and if necessary change the price to DKK.\n",
    "\n",
    "4. Create a fitting word cloud based on the `name` column. Feel free to remove non-descriptive stop words (e.g. since this is about Copenhagen, perhaps the word 'Copenhagen' is redundant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages \n",
    "\n",
    "!pip3 install wordcloud\n",
    "!pip3 install pandas\n",
    "!pip3 install folium\n",
    "!pip3 install matplotlib\n",
    "!pip3 install seaborn\n",
    "!pip3 install scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "\n",
    "airbnbSheet = pd.read_csv(\"../data/listings.csv\")\n",
    "\n",
    "desiredColumns = [\n",
    "    'id',\n",
    "    'name',\n",
    "    'host_id',\n",
    "    'host_name',\n",
    "    'neighbourhood_cleansed',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'room_type',\n",
    "    'price',\n",
    "    'minimum_nights',\n",
    "    'number_of_reviews',\n",
    "    'last_review',\n",
    "    'review_scores_rating',\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value',\n",
    "    'reviews_per_month',\n",
    "    'calculated_host_listings_count',\n",
    "    'availability_365',\n",
    "]\n",
    "\n",
    "# filteredAirbnbSheet - fas\n",
    "fas = airbnbSheet[desiredColumns]  # keep only these columns\n",
    "fas = fas.dropna()  # drop columns with empty vals\n",
    "# drop if num of reviews is 0\n",
    "fas = fas.drop(fas[fas.number_of_reviews < 1].index)\n",
    "\n",
    "\n",
    "#APPARTMENT NAMES:\n",
    "# Combine all the names into a single string\n",
    "\n",
    "text = \" \".join(name for name in fas['name'].astype(str))\n",
    "\n",
    "# List of words to remove\n",
    "stopwords = set([\"Copenhagen\", \"CPH\", \"the\", \"to\", \"and\", \"of\", \"in\", \"og\", \"i\", \"at\", \"a\"])\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"grey\").generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show() #uncomment to show the wordcloud\n",
    "\n",
    "\n",
    "airbnbSheet\n",
    "fas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Since data science is so much fun, provide a word cloud of the names of the hosts, removing any names of non-persons. Does this more or less correspond with the distribution of names according to [Danmarks Statistik](https://www.dst.dk/da/Statistik/emner/borgere/navne/navne-i-hele-befolkningen)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOST NAMES:\n",
    "\n",
    "# Combine all the host names into a single string\n",
    "text = \" \".join(name for name in fas['host_name'].astype(str))\n",
    "\n",
    "# List of words to remove (non-person names, or common words that might appear as host names)\n",
    "# This list should be expanded based on the data and common non-person names you observe\n",
    "# Replace with actual non-person names or words\n",
    "stopwords = set([\"Host1\", \"Host2\", \"ApartmentinCopenhagen\"])\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(stopwords=stopwords,\n",
    "                      background_color=\"grey\").generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a new column using bins of price. Use 11 bins, evenly distributed but with the last bin $> 10,000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the price values are in dollars\n",
    "fas = fas.rename(columns={'price': 'price_in_dollars'})\n",
    "\n",
    "# remove the dollar sign from the numbers\n",
    "fas['price_in_dollars'] = fas['price_in_dollars'].str.replace(\n",
    "    '$', '', regex=False).str.replace(',', '').astype(float)\n",
    "\n",
    "bins = [0, 1_000, 2_000, 3_000, 4_000, 5_000, 6_000,\n",
    "        7_000, 8_000, 9_000, 10_000, float('inf')]\n",
    "labels = ['$0-1,000', '$1,000-2,000', '$2,000-3,000', '$3,000-4,000', '$4,000-5,000',\n",
    "          '$5,000-6,000', '$6,000-7,000', '$7,000-8,000', '$8,000-9,000', '$9,000-10,000', '$10,000+']\n",
    "\n",
    "fas['price_bin'] = pd.cut(fas['price_in_dollars'],\n",
    "                          bins=bins, labels=labels, right=False)\n",
    "\n",
    "print(fas[['price_in_dollars', 'price_bin']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using non-scaled versions of latitude and longitude, plot the listings data on a map. Use the newly created price bins as a color parameter. Also, create a plot (i.e. another plot) where you group the listings with regard to the neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "color_map = {\n",
    "    '$0-1,000': 'green',\n",
    "    '$1,000-2,000': 'blue',\n",
    "    '$2,000-3,000': 'lightblue',\n",
    "    '$3,000-4,000': 'yellow',\n",
    "    '$4,000-5,000': 'orange',\n",
    "    '$5,000-6,000': 'darkorange',\n",
    "    '$6,000-7,000': 'orangered',\n",
    "    '$7,000-8,000': 'red',\n",
    "    '$8,000-9,000': 'darkred',\n",
    "    '$9,000-10,000': 'purple',\n",
    "    '$10,000+': 'black'\n",
    "}\n",
    "\n",
    "\n",
    "# Create a base map centered around Copenhagen\n",
    "m = folium.Map(location=[55.6761, 12.5683], zoom_start=12)\n",
    "\n",
    "# Add listings to the map using the price bins for color\n",
    "for idx, row in fas.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        color=color_map[row['price_bin']],\n",
    "        fill=True,\n",
    "        fill_color=color_map[row['price_bin']]\n",
    "    ).add_to(m)\n",
    "\n",
    "# Display the map (in Jupyter Notebook)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# Get a list of unique neighborhoods\n",
    "neighborhoods = fas['neighbourhood_cleansed'].unique()\n",
    "\n",
    "# Create a colormap\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(neighborhoods)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "neighborhood_color_map = dict(zip(neighborhoods, rainbow))\n",
    "\n",
    "\n",
    "m2 = folium.Map(location=[55.6761, 12.5683], zoom_start=12)\n",
    "\n",
    "# Add listings to the map using neighborhoods for color\n",
    "for idx, row in fas.iterrows():\n",
    "    neighborhood = row['neighbourhood_cleansed']\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        color=neighborhood_color_map[neighborhood],\n",
    "        fill=True,\n",
    "        fill_color=neighborhood_color_map[neighborhood],\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(m2)\n",
    "\n",
    "# Display the map (if you're in Jupyter Notebook)\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create boxplots where you have the neighbourhood on the x-axis and price on the y-axis. What does this tell you about the listings in Copenhagen? Keep the x-axis as is and move different variables into the y-axis to see how things are distributed between the neighborhoods to create different plots (your choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(x='neighbourhood_cleansed', y='price_in_dollars', data=fas)\n",
    "plt.yscale('log')  # Set the y-axis to a logarithmic scale\n",
    "ticks = [10, 50, 100, 500, 1000, 5000, 10000, 50000]\n",
    "plt.yticks(ticks, ticks)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Prices by Neighbourhood')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(x='neighbourhood_cleansed', y='number_of_reviews', data=fas)\n",
    "plt.yscale('log')  # Set the y-axis to a logarithmic scale\n",
    "ticks = [10, 50, 100, 200, 400, 600, 800]\n",
    "plt.yticks(ticks, ticks)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Number of Reviews by Neighbourhood')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(x='neighbourhood_cleansed', y='review_scores_rating', data=fas)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Review Scores Rating by Neighbourhood')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(x):\n",
    "    if x < 1:\n",
    "        return x * 0.05\n",
    "    elif x < 2:\n",
    "        return 0.05 + (x-1) * 0.1\n",
    "    elif x < 3:\n",
    "        return 0.15 + (x-2) * 0.15\n",
    "    elif x < 4:\n",
    "        return 0.3 + (x-3) * 0.3\n",
    "    else:\n",
    "        return 0.6 + (x-4) * 0.4\n",
    "\n",
    "def custom_inv_transform(x):\n",
    "    if x < 0.05:\n",
    "        return x / 0.05\n",
    "    elif x < 0.15:\n",
    "        return 1 + (x-0.05) / 0.1\n",
    "    elif x < 0.3:\n",
    "        return 2 + (x-0.15) / 0.15\n",
    "    elif x < 0.6:\n",
    "        return 3 + (x-0.3) / 0.3\n",
    "    else:\n",
    "        return 4 + (x-0.6) / 0.4\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(x='neighbourhood_cleansed', y=fas['review_scores_rating'].apply(custom_transform), data=fas)\n",
    "plt.yticks(ticks=[custom_transform(x) for x in [0,1,2,3,4,5]], labels=[0,1,2,3,4,5])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Review Scores Rating by Neighbourhood (Custom Scale)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a bar chart of the hosts with the top ten most listings. Place host id on the x-axis and the count of listings on the y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Needed to NOT use the calculated field for nubmer of listings, as it seems it was not the same in the filtered dataset\n",
    "\n",
    "\n",
    "# Group by host_id and count their listings\n",
    "host_counts = fas.groupby('host_id').size().sort_values(ascending=False)\n",
    "\n",
    "# Get the top 10 hosts\n",
    "top_10_hosts = host_counts.head(10)\n",
    "\n",
    "# Increase the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the vertical bar chart\n",
    "top_10_hosts.plot(kind='bar')\n",
    "\n",
    "print(top_10_hosts)\n",
    "\n",
    "# Add a title and labels to the axes\n",
    "plt.title(\"TOP 10 Hosts with the most listings\")\n",
    "plt.xlabel(\"Host id\")\n",
    "plt.ylabel(\"Number of listings\")\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the chart\n",
    "plt.tight_layout()  # Adjusts the layout for better appearance\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Do a descriptive analysis of the neighborhoods. Include information about room type in the analysis as well as one other self-chosen feature. The descriptive analysis should contain mean/average, mode, median, standard deviation/variance, minimum, maximum and quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by neighbourhood and room type\n",
    "grouped = fas.groupby(['neighbourhood_cleansed', 'room_type'])\n",
    "\n",
    "# Calculate the statistics for the 'price_in_dollars' column\n",
    "stats = grouped['price_in_dollars'].describe()\n",
    "\n",
    "# Calculate mode, since describe() doesn't provide it\n",
    "stats['mode'] = grouped['price_in_dollars'].apply(lambda x: x.mode().iloc[0])\n",
    "stats['variance'] = grouped['price_in_dollars'].var()\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Supply a list of the top 10 highest rated listings and visualize them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming fas is your DataFrame\n",
    "columns_to_average = [\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value'\n",
    "]\n",
    "\n",
    "# Now compute the mean of the specified columns for each row\n",
    "fas['review_all_scores'] = fas[columns_to_average].mean(axis=1)\n",
    "\n",
    "# Filter out listings with less reviews, to get reviews that have proper value, and are not just from listings that had few visits(ratings)\n",
    "fasWithMoreReviews = fas.drop(fas[fas.number_of_reviews < 50].index)\n",
    "\n",
    "# Now to get the top 10 highest rated listings based on this new average score:\n",
    "top_10_rated = fasWithMoreReviews.sort_values(by='review_all_scores', ascending=False).head(10)\n",
    "\n",
    "# top_10_rated['number_of_reviews']\n",
    "top_10_rated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Now, use any preprocessing and feature engineering steps that you find relevant before proceeding (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Create another new column, where the price is divided into two categories: \"expensive\" listings defined by all listings with a price higher than the median price, and \"affordable\" listings defined by all listings with a price equal to or below the median price. You can encode the affordable listings as \"0\" and the expensive ones as \"1\". All listings should now have a classification indicating either expensive listings (1) or affordable listings (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medianPrice = fas['price_in_dollars'].median()\n",
    "\n",
    "medianPrice\n",
    "\n",
    "fas['price_compared_to_median'] = \"unknown\"\n",
    "\n",
    "fas.loc[fas['price_in_dollars'] > medianPrice, 'price_compared_to_median'] = \"expensive\"\n",
    "fas.loc[fas['price_in_dollars'] <= medianPrice, 'price_compared_to_median'] = \"affordable\"\n",
    "\n",
    "print(medianPrice)\n",
    "print(fas['price_compared_to_median'])\n",
    "print(fas['price_in_dollars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Based on self-chosen features, develop a Naïve Bayes and k-Nearest Neighbor model to determine whether a rental property should be classified as 0 or 1. Remember to divide your data into training data and test data. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "\n",
    "desiredColumns = [\n",
    "    'neighbourhood_cleansed',\n",
    "    'room_type',\n",
    "    'price_compared_to_median',\n",
    "    'minimum_nights',  # for GaussianNB model these three do almost nothing in terms of accuracy score.\n",
    "    'review_all_scores',\n",
    "    'reviews_per_month'\n",
    "]\n",
    "\n",
    "dataForModel = fas[desiredColumns].copy()\n",
    "\n",
    "dataForModel = pd.get_dummies(dataForModel, columns=['neighbourhood_cleansed'])\n",
    "dataForModel = pd.get_dummies(dataForModel, columns=['room_type'])\n",
    "\n",
    "dataForModel['price_compared_to_median'] = pd.get_dummies(dataForModel['price_compared_to_median'])[\"affordable\"]\n",
    "\n",
    "dataForModel\n",
    "\n",
    "labels = dataForModel[\"price_compared_to_median\"]\n",
    "dataForModel = dataForModel.drop(['price_compared_to_median'], axis=\"columns\")\n",
    "\n",
    "\n",
    "model = GaussianNB()\n",
    "# model = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "model.fit(dataForModel, labels)\n",
    "\n",
    "print(\"Accuracy score: {}\".format(model.score(dataForModel, labels)))\n",
    "dataForModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Try to come up with a final conclusion to the Airbnb-Copenhagen assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this assignment was presumabely to learn about how is to work with a real-world larger dataset and how to get some usefull information from this data.\n",
    "\n",
    "It showed nicely how to navigate trough a large real world dataset and boil it down to simple numbers.\n",
    "Showed how to clean the data from bloat, how to come to interesting conclusions and brought some ideas on how to categorise or make sense of the data.\n",
    "Of course we also went trough sorting, making the sorting valid based on some conditions and with that also ties in the analysis of the data.\n",
    "\n",
    "\n",
    "We can conclude that there is a lot to be learned from a big dataset like this, although it has to be properly cleaned and prepared to do any analysis or any datamodeling. Also that there is much to be added and refined about a dataset such as this one, especially if we want to have some real value from it.\n",
    "\n",
    "What also can be stated that (at least) for the specific attributes I chose to use for the model, the k-Nearest Neighbor was much more accurate than Naïve Bayes in predicting if the listing is below or above the median in terms of price.\n",
    "\n",
    "There are a lot of limitations within the dataset, it doesnt tell us everything, and in most cases it only brings us the bigger picture, if we want to analyse anything closer, and want to make some predictions on something specific, its not the most accurate.\n",
    "\n",
    "But over all, the assigment has brought value in terms of gaining new knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
