{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Short-list promising models\n",
    "We expect you to do some additional research and train at **least one model per team member**.\n",
    "\n",
    "1. Train mainly quick and dirty models from different categories (e.g. linear, SVM, Random Forests etc) using default parameters\n",
    "2. Measure and compare their performance\n",
    "3. Analyse the most significant variables for each algorithm\n",
    "4. Analyse the types of errors the models make\n",
    "5. Have a quick round of feature selection and engineering if necessary\n",
    "6. Have one or two more quick iterations of the five previous steps\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/jaa/Library/Python/3.9/lib/python/site-packages (2.1.0)\n",
      "Requirement already satisfied: keras in /Users/jaa/Library/Python/3.9/lib/python/site-packages (2.14.0)\n",
      "Requirement already satisfied: torch in /Users/jaa/Library/Python/3.9/lib/python/site-packages (2.1.1)\n",
      "Requirement already satisfied: tqdm in /Users/jaa/Library/Python/3.9/lib/python/site-packages (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: filelock in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/jaa/Library/Python/3.9/lib/python/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in /Users/jaa/Library/Python/3.9/lib/python/site-packages (0.16.1)\n",
      "Requirement already satisfied: torchaudio in /Users/jaa/Library/Python/3.9/lib/python/site-packages (2.1.1)\n",
      "Requirement already satisfied: networkx in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: filelock in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: fsspec in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: numpy in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jaa/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# installing dependencies\n",
    "!pip3 install pandas keras torch tqdm\n",
    "\n",
    "# !pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data as dataframes\n",
    "import pandas as pd\n",
    "\n",
    "emotions_df_train_ready = pd.read_csv('../data/emotions_train_ready.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### just to not forget the mapping\n",
    "\n",
    "- 0: 'sadness'\n",
    "- 1: 'joy'\n",
    "- 2: 'love'\n",
    "- 3: 'anger'\n",
    "- 4: 'fear'\n",
    "- 5: 'surprise'\n",
    "- 6: 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0  text                                                                                                                                                                                                            label\n",
       "15          [NAME] is always the dirtiest psychopath                                                                                                                                                                        3        1\n",
       "400306      i have found the more i suppress the vampire inside me the more human emotions i feel the days i don t feel like feeling anymore are the days i m dangerous                                                     3        1\n",
       "400224      i describe his gentle nature and how being in his presence made me feel more tranquil                                                                                                                           1        1\n",
       "400225      Man, what a very tolerant person he is.                                                                                                                                                                         6        1\n",
       "400234      i dont recall just now yet vividly recall looking at you as you said it and you i think looking back at me and my feeling very sympathetic or maybe empathetic is the better word of course you needed a space  2        1\n",
       "                                                                                                                                                                                                                                    ..\n",
       "200306      i know what that feels like and i hate it so i try to be considerate and listen to them                                                                                                                         2        1\n",
       "200307      sweet home alabama                                                                                                                                                                                              1        1\n",
       "200308      i am seriously at a point where i feel that nothing i do is worthwhile and i figure if i can feel this way i m betting plenty of other people have those feelings at one point or another                       1        1\n",
       "200320      i could feel my etheric hands plunge into this rich soil sending healing and nurturing                                                                                                                          1        1\n",
       "598922      The crime here is using the shaker parmesan crap.                                                                                                                                                               6        1\n",
       "Name: count, Length: 59897, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_df_train_ready.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'arent', 'feeling', 'too', 'sleep', 'deprived']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Custom tokenization using regular expressions\n",
    "def custom_tokenize(text):\n",
    "    # Regular expression for splitting on whitespace and keeping punctuation\n",
    "    tokens = re.findall(r\"\\b\\w+\\b|[!?.]\", text)\n",
    "    return tokens\n",
    "\n",
    "# Applying custom tokenization to the entire dataset\n",
    "tokenized_texts = [custom_tokenize(text) for text in emotions_df_train_ready['text']]\n",
    "\n",
    "# Example of tokenized text\n",
    "tokenized_texts_example = tokenized_texts[0]\n",
    "tokenized_texts_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of tokenized texts\n",
    "all_tokens = [token for text in tokenized_texts for token in text]\n",
    "vocabulary = set(all_tokens)\n",
    "\n",
    "# Mapping tokens to integers\n",
    "token_to_int = {token: i+1 for i, token in enumerate(vocabulary)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum vocabulary size\n",
    "max_vocab_size = 10000  # You can adjust this number as needed\n",
    "\n",
    "# Count the frequency of each token\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Keep only the most frequent tokens\n",
    "most_common_tokens = [token for token, count in token_counts.most_common(max_vocab_size)]\n",
    "\n",
    "# Create a new token-to-integer mapping, including a token for unknown words\n",
    "token_to_int = {token: i+1 for i, token in enumerate(most_common_tokens)}\n",
    "token_to_int[\"<UNK>\"] = len(most_common_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the tokenized texts using the updated mapping\n",
    "encoded_sequences = []\n",
    "for text in tokenized_texts:\n",
    "    encoded_text = [token_to_int.get(token, token_to_int[\"<UNK>\"]) for token in text]\n",
    "    encoded_sequences.append(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Finding the maximum sequence length\n",
    "max_seq_length = max(len(seq) for seq in encoded_sequences)\n",
    "\n",
    "# Padding the sequences\n",
    "padded_sequences = np.array([seq + [0]*(max_seq_length - len(seq)) for seq in encoded_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the labels\n",
    "labels = emotions_df_train_ready['label'].values\n",
    "\n",
    "# One-hot encoding the labels\n",
    "num_classes = len(np.unique(labels))\n",
    "one_hot_labels = np.eye(num_classes)[labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  1,  68,  55, ...,   0,   0,   0],\n",
       "        [  1,  20,  90, ...,   0,   0,   0],\n",
       "        [ 11, 235, 286, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  1, 308,  31, ...,   0,   0,   0],\n",
       "        [  1,  22,  35, ...,   0,   0,   0],\n",
       "        [  1,  20, 238, ...,   0,   0,   0]]),\n",
       " array([[  17,    2,   15, ...,    0,    0,    0],\n",
       "        [   1,  156,  326, ...,    0,    0,    0],\n",
       "        [   1,    2,  217, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1,   20,  319, ...,    0,    0,    0],\n",
       "        [1062,   19, 1107, ...,    0,    0,    0],\n",
       "        [  26,   18,  310, ...,    0,    0,    0]]),\n",
       " array([[   1, 2132,  107, ...,    0,    0,    0],\n",
       "        [   1,   68,    3, ...,    0,    0,    0],\n",
       "        [ 366,   19,  223, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  26,  345,   53, ...,    0,    0,    0],\n",
       "        [2015,    7,  171, ...,    0,    0,    0],\n",
       "        [   1,    2,  764, ...,    0,    0,    0]]),\n",
       " array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]),\n",
       " array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into training and the rest\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    padded_sequences, one_hot_labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Further splitting the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the train val test split so we can work in multiple files:\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the arrays to a file\n",
    "with open('../data/emption_dataset_partitions_train_ready.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, X_val, X_test, y_train, y_val, y_test), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2 = self.multihead_attn(src, src, src, attn_mask=src_mask,\n",
    "                                   key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.transformer_layers = nn.ModuleList([TransformerBlock(d_model, nhead, nhid, dropout) for _ in range(nlayers)])\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        for mod in self.transformer_layers:\n",
    "            src = mod(src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "       # Select the output of the first token for classification\n",
    "        src = src[:, 0, :]\n",
    "        output = self.decoder(src)\n",
    "        return output\n",
    "\n",
    "\n",
    "def createTransformer(hyperparams: dict):\n",
    "    model = TransformerModel(\n",
    "            hyperparams['ntokens'],\n",
    "            hyperparams['d_model'],\n",
    "            hyperparams['nhead'],\n",
    "            hyperparams['nhid'],\n",
    "            hyperparams['nlayers'],\n",
    "            hyperparams['dropout']\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying other tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "hyperparams_v1 = {\n",
    "    'ntokens': len(most_common_tokens) + 2, # size of vocabulary\n",
    "    'd_model': 128, # embedding dimension\n",
    "    'nhead': 4    , # number of heads in multi-head attention models\n",
    "    'nhid': 512   ,# dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    'nlayers': 4  , # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    'dropout': 0.1, # dropout probability\n",
    "}\n",
    "\n",
    "batch_size = 32\n",
    "vobab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "\n",
    "ntokens = len(most_common_tokens) + 2 # size of vocabulary\n",
    "d_model = 128 # embedding dimension\n",
    "nhead = 4     # number of heads in multi-head attention models\n",
    "nhid = 512   # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4   # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.1 # dropout probability\n",
    "\n",
    "model = createTransformer(hyperparams_v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first iteration of loader for old data splits\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = TextDataset(X_train, y_train)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, texts, max_vocab_size):\n",
    "        self.vocab = self.build_vocab(texts, max_vocab_size)\n",
    "        self.word_index = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.index_word = {i: word for word, i in self.word_index.items()}\n",
    "\n",
    "    def build_vocab(self, texts, max_vocab_size):\n",
    "        # Count word frequencies\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        # Sort words by frequency and take the top 'max_vocab_size' words\n",
    "        vocab = [word for word, freq in sorted(word_freq.items(), key=lambda x: -x[1])]\n",
    "        vocab = vocab[:max_vocab_size]\n",
    "\n",
    "        # Add special tokens\n",
    "        vocab = ['<pad>', '<unk>'] + vocab\n",
    "        return vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.word_index.get(word, self.word_index['<unk>']) for word in text.split()]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        return ' '.join([self.index_word.get(index, '<unk>') for index in sequence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = emotions_df_train_ready\n",
    "\n",
    "tokenizer = Tokenizer(data['text'], max_vocab_size=10000)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = [tokenizer.encode(text) for text in data['text']]\n",
    "\n",
    "# Padding sequences\n",
    "max_length = 128  # or another value that suits your model\n",
    "padded_sequences = np.array([np.pad(seq, (0, max_length - len(seq)), mode='constant') for seq in sequences])\n",
    "\n",
    "# One-hot encode labels\n",
    "labels = np.array(pd.get_dummies(data['label']))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
    "X_val, y_val = torch.tensor(X_val), torch.tensor(y_val)\n",
    "X_test, y_test = torch.tensor(X_test), torch.tensor(y_test)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 25,  55,  55,  ...,   0,   0,   0],\n",
       "         [ 27,  65,  50,  ...,   0,   0,   0],\n",
       "         [  2,   3, 664,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  2,  65,   7,  ...,   0,   0,   0],\n",
       "         [  2,   3, 417,  ...,   0,   0,   0],\n",
       "         [  2,   3,   9,  ...,   0,   0,   0]]),\n",
       " tensor([[False, False, False,  ...,  True, False, False],\n",
       "         [ True, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ...,  True, False, False],\n",
       "         ...,\n",
       "         [ True, False, False,  ..., False, False, False],\n",
       "         [ True, False, False,  ..., False, False, False],\n",
       "         [False,  True, False,  ..., False, False, False]]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "## Switch to M! GPU acceleration \n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if MPS (Apple's M1 GPU) is available and use it; otherwise, use CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 7])\n"
     ]
    }
   ],
   "source": [
    "# checking if we have the correct output from the model\n",
    "\n",
    "sequence_length = 50\n",
    "\n",
    "dummy_input = torch.randint(0, ntokens, (sequence_length, batch_size))\n",
    "dummy_output = model(dummy_input.to(device))\n",
    "print(dummy_output.shape)  # Should be [batch_size, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model_if_not_exists(model: TransformerModel, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File '{file_path}' already exists. Model not saved to avoid overwriting.\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "        print(f\"Model saved successfully to '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.long().to(device), torch.argmax(labels.float(), dim=1).long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}', flush=True)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.long().to(device), torch.argmax(labels.float(), dim=1).long().to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}', flush=True)\n",
    "    return all_true_labels, all_predictions\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example of a function to print classification report\n",
    "def print_classification_report(true_labels, predictions):\n",
    "    print(classification_report(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jaa/Library/Mobile Documents/com~apple~CloudDocs/Documents/VIA/7th semester/MAL1Y - Monday/mal1y/final-group-project/src/5-training-models.ipynb Cell 33\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaa/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/VIA/7th%20semester/MAL1Y%20-%20Monday/mal1y/final-group-project/src/5-training-models.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jaa/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/VIA/7th%20semester/MAL1Y%20-%20Monday/mal1y/final-group-project/src/5-training-models.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_one_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jaa/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/VIA/7th%20semester/MAL1Y%20-%20Monday/mal1y/final-group-project/src/5-training-models.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     true_labels, predictions \u001b[39m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jaa/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/VIA/7th%20semester/MAL1Y%20-%20Monday/mal1y/final-group-project/src/5-training-models.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     print_classification_report(true_labels, predictions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jaa/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/VIA/7th%20semester/MAL1Y%20-%20Monday/mal1y/final-group-project/src/5-training-models.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m## currently 8 epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jaa/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/VIA/7th%20semester/MAL1Y%20-%20Monday/mal1y/final-group-project/src/5-training-models.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Save the model's state dictionary\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "num_epochs = 8\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    true_labels, predictions = validate(model, val_loader, criterion, device)\n",
    "    print_classification_report(true_labels, predictions)\n",
    "\n",
    "## currently 8 epochs\n",
    "# Save the model's state dictionary\n",
    "save_model_if_not_exists(model, './models/emotions_transformer_model_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model (ensure it has the same architecture as the saved one)\n",
    "loaded_model = createTransformer(hyperparams_v1)\n",
    "\n",
    "\n",
    "loaded_state_dict = torch.load('./models/emotions_transformer_model_2.pth')\n",
    "\n",
    "loaded_model.load_state_dict(loaded_state_dict)\n",
    "\n",
    "# If you are using the MPS device\n",
    "loaded_model = loaded_model.to(device)\n",
    "\n",
    "loaded_model.eval()\n",
    "\n",
    "loaded_model = loaded_model.to(device).float()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TextDataset(X_val, y_val)  # Assuming TextDataset is your custom dataset class\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Lists to store true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "loaded_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc='Validating', leave=False):\n",
    "        inputs = inputs.to(device).long()\n",
    "        \n",
    "        # Assuming labels are one-hot encoded\n",
    "        true_class_indices = np.argmax(labels.cpu().numpy(), axis=1)\n",
    "        \n",
    "        outputs = loaded_model(inputs)\n",
    "        # print('🚀 ~ file: 5-training-models.ipynb:18 ~ outputs.shape:', outputs.shape)\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)  # Convert outputs to class indices\n",
    "        # print('🚀 ~ file: 5-training-models.ipynb:21 ~ preds.shape:', preds.shape)\n",
    "        \n",
    "        add_to_preds = preds.cpu().numpy().tolist()\n",
    "        # print('🚀 ~ file: 5-training-models.ipynb:21 ~ add_to_preds:', add_to_preds)\n",
    "        \n",
    "        add_to_true = true_class_indices.tolist()\n",
    "        # print('🚀 ~ file: 5-training-models.ipynb:21 ~ add_to_true:', add_to_true)\n",
    "        \n",
    "        \n",
    "        true_labels.extend(add_to_true)  # Convert to list if not already\n",
    "        predicted_labels.extend(add_to_preds)  # Convert to list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3336\n",
      "Validation Precision: 0.0477\n",
      "Validation Recall: 0.1429\n",
      "Validation F1 Score: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaa/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {precision:.4f}\")\n",
    "print(f\"Validation Recall: {recall:.4f}\")\n",
    "print(f\"Validation F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
